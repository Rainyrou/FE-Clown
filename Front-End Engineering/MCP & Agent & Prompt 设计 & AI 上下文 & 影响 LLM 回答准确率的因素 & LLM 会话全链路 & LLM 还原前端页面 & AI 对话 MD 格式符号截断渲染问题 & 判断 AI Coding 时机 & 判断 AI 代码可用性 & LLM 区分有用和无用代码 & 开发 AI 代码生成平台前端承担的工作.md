MCP：LLM 的上下文标准化交互协议，将所有类型的 AI 上下文抽象为标准化最小单元 Context Unit，该单元为协议载体，包含全局唯一 context_id、模态类型、会话关联和优先级等元数据，及模态适配的标准化编码内容，通过增量传输、Protocol Buffer/Flat Buffer 二进制序列化和分级缓存实现上下文交互和性能优化，当 AI 上下文超出模型窗口阈值则触发优先级截断和语义压缩

Agent：将 LLM 从概率统计文本生成器升级为有环境感知与逻辑决策能力的控制器，遵循"感知 - 规划 - 行动 - 记忆" 机制将目标拆分为可执行的子任务序列，通过 AI 上下文窗口维持短期记忆并结合向量数据库和 Embedding 技术实现长期记忆的持久存储与语义检索，基于 Reasoning + Acting 范式或 Function Calling 机制，将自然语言描述映射为结构化 API 调用指令，调度外部工具与外部环境进行交互并将执行结果作为新上下文返回给模型以修正下一步策略，从而形成闭环，直至完成目标或触发终止条件

Prompt 设计：

- 结构化分层：根据"领域规则 → 输出格式 → 示例锚定 -> 增量迭代"分层拆解 Prompt，各层独立成段且优先级明确
- 领域规则：输入垂直领域的核心规则，激活 LLM 预训练参数存储的垂直领域知识
- 输出格式：采用量化的规则替代模糊自然语言描述并明确"禁止生成"的边界
- 示例锚定：输入少量与目标输出高度相似的示例，让 LLM 对齐输出格式
* 增量迭代：增量修正 Prompt 聚焦偏差点,让 LLM 聚焦于偏差 Token 而非全量销毁创建
* 安全合规：判断并阻断 Prompt 注入攻击，通过正则匹配 + 向量检索比对敏感词库以确保 Prompt 合规；拦截越权 Prompt，将 Prompt 的权限诉求与用户身份凭证进行前置校验；对 Prompt 进行敏感信息脱敏处理，对生成内容进行二次校验；隔离不同用户的 Prompt 与上下文，加密存储与访问控制 Prompt 的历史记录

AI 上下文是 LLM 推理时依赖的结构化历史信息集合

- 会话 ID + 创建时间戳、需求类型和模型窗口阈值等
- 平台预设或用户自定义的模型行为约束
- 用户输入 + 模型响应的多轮对话有序集合
- 资源索引 Token

影响 LLM 回答准确率的因素：预训练 + 微调 + Prompt + 上下文 + 模型架构 + 推理参数 + 外部补充 + 幻觉控制

LLM 会话全链路：

- 新需求触发时，服务端生成全局唯一会话 ID 并与用户身份凭证绑定，通过权限验证用户资源配额（如 GPT 的 token 限额 + Cursor 的文件上传大小限制 + Gemini 的多模态处理时长）、会话隔离性和接口调用合法性，校验失败则拦截请求
- 初始化会话上下文，采用 LRU 策略，以键值对形式存储会话 ID 和上下文元数据（创建时间戳、需求类型和模型窗口阈值等）
- 历史会话通过模型算法转换为数值 Token，通过正则匹配 + 向量检索比对敏感词库以确保用户输入合规
- 上下文分层存储，根据优先级从高到低依次为系统提示词 -> 用户当前输入 -> 关联历史上下文 -> 资源索引 Token，通过滑动窗口 + 优先级排序机制，实时计算上下文 Token 总数，超出模型窗口阈值则触发优先级截断和语义压缩
- 资源分片上传，通过规则引擎过滤非法资源，验证资源格式与模型兼容性，资源存储于分布式对象存储，生成唯一资源 URL 并与会话 ID 绑定
- 客户端将会话 ID + 结构化上下文封装为 API 请求并发送给服务端，服务端接收到请求后，通过会话 ID 检索上下文容器，验证资源索引的有效性及 Token 总数是否超出模型窗口阈值，同时将结构化 Token 序列输入模型推理引擎触发前向计算生成响应 Token，将本次用户需求与模型响应追加至上下文容器，更新 Token 总数与时间戳

LLM 还原前端页面：本质为「多模态语义特征 → 前端语法 Token → 工程化代码」的映射过程，采集前端页面的结构化量化特征，明确前端技术栈、页面布局、原子样式和业务逻辑，对应 LLM 参数中存储的前端语法规则，UI 稿通过编码器转换为颜色 Token 与文本描述的语义 Token 对齐，即视觉特征映射为 CSS 语法，对页面布局、组件层级和原子属性分层解析，将语义特征转换为前端语法，研发对齐 UI 稿和实际效果，结构化描述偏差点置于 Prompt，强调前端工程规范约束，给出少量样本代码示例，让 LLM 聚焦于偏差 Token 而非全量销毁创建

AI 上下文 MD 格式语法截断渲染问题：

* 生成内容以原子语法节点为最小截断单位，判断节点边界，确保截断点位于节点间而非节点内部
* 定义截断符号优先级规则，遍历 AST，识别未闭合节点和单独符号，补全块级节点的闭合符号，将单独符号降级为文本，为无法修复的节点添加截断标记，将块级残缺节点降级为"文本+空格/换行基础格式"，将行内残缺节点降级为文本并全局捕获渲染异常，在保证文本可读性的基础上保留语法格式

判断 AI Coding 时机：权衡成本和收益，AI Coding 的效率能否 Cover 其校验、修正和风险成本，AI Coding 适用于短链路、低风险、Prompt 可控的简单任务，研发 Coding 适用于长链路、创新性、核心业务的复杂任务

判断 AI 代码可用性：

- 前端语法：通过语法引擎解析 AI 代码 AST，验证是否存在语法错误
- 业务逻辑：测试 AI 代码并验证输出结果是否符合预期；将当前需求和代码文件转换为语义向量，计算需求向量和代码向量的余弦相似度并设置阈值，相似度大于等于阈值的代码标记为有用代码，否则标记为无用代码
- 兼容适配：前端工程化验证 AI 代码是否符合规范；验证 AI 代码是否适配目标平台
- 安全合规：判断 AI 代码是否存在越权逻辑；通过第三方工具解析 AI 代码 AST，判断是否存在安全风险
- 性能优化：在开发和生产环境测试网络和页面层面的性能指标指标

LLM 区分有用和无用代码：

- 分层标注：根据前端工程化定义有用/无用的判断标准，通过 AST 提取结构化特征实现自动标注，标注与当前需求或业务链路直接/间接关联的代码为核心/边缘有用代码，标注与当前需求或业务链路无关联的代码为无用代码
- 向量关联：将当前需求和代码文件转换为语义向量，计算需求向量和代码向量的余弦相似度并设置阈值，相似度大于等于阈值的代码标记为有用代码，否则标记为无用代码
- 代码索引：生成有用代码的轻量索引，保留核心有用代码的 AST 关键节点 + 代码片段哈希值，保留边缘有用代码的模块名 + 功能描述
- 规则过滤：从文件类型、代码特征和业务链路的角度设置无用代码的过滤规则并进行过滤
- 优先级分配：根据有用代码的优先级置于 LLM 上下文并分配相应权重

开发 AI 代码生成平台前端承担的工作：

- 前端通过后端下发的权限列表控制某些代码仓库和提交信息等的显示隐藏，通过配置用户权限让其仅能访问对应权限页面，颗粒度，用户 -> 角色 -> 菜单 -> 操作（1：n）
- 开发基于 Iframe/微前端的隔离沙箱环境，模拟浏览器和 Node.js 运行时，执行生成代码并捕获运行时错误，将运行时错误映射至生成代码的具体行号，结合 AST 解析定位错误根源
- 生成代码开始/暂停/继续/重试的控制能力，对接后端 SSE 或 WebSocket 协议，流式解析返回的代码 Token，根据语法规则逐段拼接并渲染
- 实现文本 Prompt 编辑器的语法高亮、自动补全和模板输入，对接多模态输入如图片上传和语音输入，图片自适应，语音转换为文本的语义过滤转义清洗
- 可视化会话 ID + 创建时间戳、需求类型和模型窗口阈值等 + 平台预设或用户自定义的模型行为约束 + 用户输入 + 模型响应的多轮对话有序集合 + 资源索引 Token，LLM 区分有用和无用代码的笔记
